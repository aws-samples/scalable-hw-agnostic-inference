# scalable-hw-agnostic-inference

The growing interest in gen-AI applications is driving demand for compute accelerators, resulting in increased inference costs and a shortage of compute capacity. This has led to the introduction of more compute accelerator options, such as NVIDIA and Inferentia. However, each option introduces novel methods for running AI applications on the compute accelerator and requires different code implementations, such as Neuron and CUDA SDKs. Here, we present methods to reduce AI accelerator and CPU costs and minimize compute capacity constraints. We explore the benefits of NVIDIA's GPUs and AWS Inferentia instances for a PyTorch-based sample GenAI model called Stable Diffusion, hosted on HuggingFace. As part of our benchmarking technique, we demonstrate a gradual migration strategy controlled by K8s ingress controllers during routing time and Karpenter during scheduling time. Additionally, we show how to scale K8s deployment size based on critical metrics published to CloudWatch, such as inference latency and throughput.


