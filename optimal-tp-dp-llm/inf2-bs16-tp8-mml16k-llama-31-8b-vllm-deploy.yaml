apiVersion: v1
kind: Service
metadata:
  name: inf2-bs16-tp8-mml16k-llama-31-8b-vllm
spec:
  selector:
    app: inf2-bs16-tp8-mml16k-llama-31-8b-vllm
  ports:
    - port: 8000
      targetPort: 8000
  type: NodePort
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: inf2-bs16-tp8-mml16k-llama-31-8b-vllm
    aws.amazon.com/neuron: "4"
  name: inf2-bs16-tp8-mml16k-llama-31-8b-vllm
spec:
  selector:
    matchLabels:
      app: inf2-bs16-tp8-mml16k-llama-31-8b-vllm
  template:
    metadata:
      labels:
        app: inf2-bs16-tp8-mml16k-llama-31-8b-vllm
        aws.amazon.com/neuron: "4"
    spec:
      nodeSelector:
        karpenter.sh/nodepool: neuron-inf2 
      serviceAccountName: appsimulator
      schedulerName: my-scheduler
      volumes:
      - name: dshm
        emptyDir:
          medium: Memory
      containers:
      - name: app
        image: 975050117197.dkr.ecr.us-west-2.amazonaws.com/model:amd64-neuron
        imagePullPolicy: Always
        volumeMounts:
          - mountPath: /vllm_config.yaml
            name: vllm-config-volume
            subPath: vllm_config.yaml
          - mountPath: /dev/shm
            name: dshm
        command: 
        - /bin/bash
        - "-exc"
        - |
          set -x
          pip install --upgrade pip
          pip config set global.extra-index-url https://pip.repos.neuron.amazonaws.com
          pip install --upgrade "neuronx-distributed==0.11.0" --extra-index-url https://pip.repos.neuron.amazonaws.com
          pip install --upgrade "neuronx-distributed-inference>=0.2.0" --extra-index-url https://pip.repos.neuron.amazonaws.com
          pip install --upgrade transformers accelerate protobuf sentence_transformers tenacity
          git clone -b neuron-2.22-vllm-v0.7.2 https://github.com/aws-neuron/upstreaming-to-vllm.git
          cd upstreaming-to-vllm
          pip install -r requirements-neuron.txt
          VLLM_TARGET_DEVICE="neuron" pip install -e .
          cd /
          python /download_hf_model.py
          uvicorn vllm_model_api:app --host=0.0.0.0
        resources:
          requests:
            aws.amazon.com/neuron: 4
            #memory: 240Gi
          limits:
            aws.amazon.com/neuron: 4
            #memory: 240Gi
        env:
        - name: MODEL_ID
          value: "yahavb/inf2-bs16-tp8-mml16k-llama-31-8b-vllm"
          #value: "meta-llama/Llama-3.1-8B"
        - name: VLLM_NEURON_FRAMEWORK
          value: "neuronx-distributed-inference"
        - name: APP
          value: "inf2-bs16-tp8"
        - name: NODEPOOL
          value: "amd-neuron-inf2" 
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: HUGGINGFACE_TOKEN
          valueFrom:
            secretKeyRef:
              name: hf-secrets
              key: HUGGINGFACE_TOKEN
        - name: MAX_NEW_TOKENS
          value: "512"
        ports:
          - containerPort: 8000
            protocol: TCP
        readinessProbe:
          failureThreshold: 3
          httpGet:
            path: /readiness
            port: 8000
            scheme: HTTP
          initialDelaySeconds: 480
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 5
      volumes:
      - name: vllm-config-volume
        configMap:
          name: inf2-bs16-tp8-mml16k-llama-31-8b-vllm
      - name: dshm
        emptyDir:
          medium: Memory
