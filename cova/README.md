# âš¡â€¯Image generation va;idation (cova) Demo â€“ Multimodal Inference on NxD (EKS centric)

A Gradio web app that chains **three model microâ€‘services**:

| order | microâ€‘service | purpose |
|-------|---------------|---------|
| â‘  | **Flux Imageâ€‘Gen** | turns a text prompt into a PNG | 
| â‘¡ | **Llamaâ€¯3.2 Captioner** | describes the generated image |
| â‘¢ | **T5 Encoder** | embeds (a) the caption and (b) the original prompt |

All four pieces (â‘ â€“â‘¢ + the Gradio orchestrator) are independent Kubernetes
Deployments, each fronted by a ClusterIPÂ Service.  
The Gradio pod fansâ€‘out asynchronous calls and renders latenciesÂ + outputs.

---

## ğŸ–¼ï¸Â Architecture

```mermaid
flowchart TD
    subgraph InferenceÂ Pipeline
        direction LR
        A[GradioÂ UI<br/>`covaâ€‘gradio`] -- prompt --> IMG[FluxÂ ImageÂ Gen]
        IMG -- PNG --> CAP[LlamaÂ 3.2Â Captioner]
        CAP -- caption&nbsp;txt --> ENC[T5Â Encoder]
    end

    IMG -- image&nbsp;+&nbsp;latency --> A
    CAP -- caption&nbsp;+&nbsp;latency --> A
    ENC -- embeddings&nbsp;+&nbsp;latency --> A
```

## Files in this repo

| file | kind | what it defines |
|------|------|-----------------|
| `cova-gradio-config.yaml` | ConfigMap | **models.json** consumed by the Gradio pod |
| `cova-gradio-deploy.yaml` | Deployment + Service | Gradio orchestrator |
| `mllama-32-11b-vllm-trn1-config.yaml` | ConfigMap | HFÂ token / Neuron overrides for the captioner |
| `mllama-32-11b-vllm-trn1-deploy.yaml` | Deployment + Service | vLLM caption model |
| `t5-neuron-model-api.yaml` | Deployment + Service | T5 encoder |
| `cova-ingress.yaml` | Ingress / IngressRoute | optional public entryâ€‘point |
| `cova_gradio_m.py` | source | orchestrator code (imageÂ â†’ captionÂ â†’ embeddings) |


## ğŸ”§Â Key configuration (models.json)

```json
[
  {
    "name": "512 Ã— 512",
    "host_env":               "FLUX_NEURON_512X512_MODEL_API_SERVICE_HOST",
    "port_env":               "FLUX_NEURON_512X512_MODEL_API_SERVICE_PORT",
    "height": 512,
    "width":  512,

    "caption_host_env":       "MLLAMA_32_11B_VLLM_TRN1_SERVICE_HOST",
    "caption_port_env":       "MLLAMA_32_11B_VLLM_TRN1_SERVICE_PORT",
    "caption_max_new_tokens": 1024,

    "encoder_host_env":       "T5_NEURON_MODEL_API_SERVICE_HOST",
    "encoder_port_env":       "T5_NEURON_MODEL_API_SERVICE_PORT",
    "encoder_max_new_tokens": 256
  }
]
```

## ğŸš€Â Deploy

```bash
# 1Â â€“Â ConfigMaps
kubectl apply -f cova-gradio-config.yaml
kubectl apply -f mllama-32-11b-vllm-trn1-config.yaml

# 2Â â€“Â Model backâ€‘ends
kubectl apply -f mllama-32-11b-vllm-trn1-deploy.yaml
kubectl apply -f t5-neuron-model-api.yaml

# 3Â â€“Â Frontâ€‘end
kubectl apply -f cova-gradio-deploy.yaml

# 4Â â€“Â Ingress (optional)
kubectl apply -f cova-ingress.yaml
```

## ğŸƒ Smoke test (curl)

```bash
curl -X POST http://$T5_NEURON_MODEL_API_SERVICE_HOST:$T5_NEURON_MODEL_API_SERVICE_PORT/generate \
     -H 'Content-Type: application/json'                       \
     -d '{"prompt": "Hello world", "max_new_tokens": 32}' | jq
```

Outputs Base64â€‘encoded embedding string + latency in seconds.

![Cova (Content Validation) UI â€“ image, caption and embeddings sideâ€‘byâ€‘side](./app-demo.png)

We can see the time it took to generate the image (5.61s), the caption (5.70s), the caption embeddings (0.20s) and the prompt embeddings (0.09s). 

## ğŸ§¹Â Cleanup

```bash
kubectl delete -f cova-ingress.yaml --ignore-not-found
kubectl delete -f cova-gradio-deploy.yaml
kubectl delete -f t5-neuron-model-api.yaml
kubectl delete -f mllama-32-11b-vllm-trn1-deploy.yaml
kubectl delete -f cova-gradio-config.yaml
kubectl delete -f mllama-32-11b-vllm-trn1-config.yaml
```
